import os
from typing import Any, Callable, Dict, Optional, Type, cast
import logging
from pydantic import BaseModel
from abc import ABC, abstractmethod
from guardrails.validator_base import (
    FailResult,
    PassResult,
    ValidationResult,
    Validator,
    register_validator,
)

from guardrails.stores.context import get_call_kwarg
from litellm import completion, get_llm_provider
from validator.enums import TextLanguage
from validator.prompt_hub import RAG_CONTEXT_RELEVANCE_PROMT
from validator.models import RagRelevanceResponse

logger = logging.getLogger(__name__)


class Ml3RagEvaluationPromptBase(ABC):
    def __init__(self, prompt_name, **kwargs) -> None:
        self.prompt_name = prompt_name

    @abstractmethod
    def generate_prompt(
        self,
        user_input: str | None,
        retrieved_context: str | None,
        llm_response: str | None,
        min_range_value: int = 1,
        max_range_value: int = 5,
        language: TextLanguage = TextLanguage.ENGLISH,
    ) -> str:
        pass


class RagContextRelevancePrompt(Ml3RagEvaluationPromptBase):
    def generate_prompt(
        self,
        user_input: str | None,
        retrieved_context: str | None,
        llm_response: str | None,
        min_range_value: int = 1,
        max_range_value: int = 5,
        language: TextLanguage = TextLanguage.ENGLISH,
    ) -> str:
        return RAG_CONTEXT_RELEVANCE_PROMT.format(
            user_input=user_input,
            retreived_context=retrieved_context,
            min_range_value=min_range_value,
            max_range_value=max_range_value,
            language=language,
        )


@register_validator(name="ml3/llm_rag_relevance_evaluator", data_type="string")
class LlmRagRelevanceEvaluator(Validator):
    """This class validates an output generated by a LiteLLM (LLM) model by prompting another LLM model to evaluate the output.

    **Key Properties**

    | Property                      | Description                       |
    | ----------------------------- | --------------------------------- |
    | Name for `format` attribute   | `ml3/relevancy_evaluator`         |
    | Supported data types          | `string`                          |
    | Programmatic fix              | N/A                               |

    Args:
        llm_callable (str, optional): The name of the LiteLLM model to use for validation. Defaults to "gpt-3.5-turbo".
        on_fail (Callable, optional): A function to be called when validation fails. Defaults to None.
    """

    def __init__(
        self,
        eval_llm_prompt_generator: Ml3RagEvaluationPromptBase,
        response_format: RagRelevanceResponse,
        pass_threshold: int,
        llm_callable: str,
        on_fail: Optional[Callable] = "noop",
        **kwargs,
    ):
        super().__init__(
            on_fail,
            eval_llm_prompt_generator=eval_llm_prompt_generator,
            llm_callable=llm_callable,
            **kwargs,
        )
        self._llm_evaluator_prompt_generator = eval_llm_prompt_generator
        self._llm_callable = llm_callable
        self._response_format = response_format
        self._pass_threshold = pass_threshold

    def get_llm_response(self, prompt: str) -> str:
        """Gets the response from the LLM.

        Args:
            prompt (str): The prompt to send to the LLM.

        Returns:
            str: The response from the LLM.
        """
        # 0. Create messages
        messages = [{"content": prompt, "role": "user"}]

        # 0b. Setup auth kwargs if the model is from OpenAI
        kwargs = {}
        _model, provider, *_rest = get_llm_provider(self._llm_callable)
        if provider == "openai":
            kwargs["api_key"] = get_call_kwarg("api_key") or os.environ.get(
                "OPENAI_API_KEY"
            )

        # 1. Get LLM response
        # Strip whitespace and convert to lowercase
        try:
            response = completion(
                model=self._llm_callable,
                messages=messages,
                response_format=self._response_format,
                **kwargs,
            )
            response = response.choices[0].message.content  # type: ignore
        except Exception as e:
            raise RuntimeError(f"Error getting response from the LLM: {e}") from e

        # 3. Return the response
        return response

    def validate(self, value: Any, metadata: Dict) -> ValidationResult:
        """
        Validates is based on the relevance of the reference text to the original question.

        Args:
            value (Any): The value to validate. It must contain 'original_prompt' and 'reference_text' keys.
            metadata (Dict): The metadata for the validation.
                user_message: Required key. User query passed into RAG LLM.
                context: Required key. Context used by RAG LLM.
                llm_response: Optional key. By default, the gaurded LLM will make the RAG LLM call, which corresponds
                    to the `value`. If the user calls the guard with on="prompt", then the original RAG LLM response
                    needs to be passed into the guard as metadata for the LLM judge to evaluate.

        Returns:
            ValidationResult: The result of the validation. It can be a PassResult if the reference
                              text is relevant to the original question, or a FailResult otherwise.
        """
        # 1. Get the question and arg from the value
        user_input = metadata.get("user_input")
        if user_input is None:
            raise RuntimeError(
                "user_input missing from value. Please provide the original prompt."
            )

        retrieved_context = metadata.get("retrieved_context")
        if retrieved_context is None:
            raise RuntimeError(
                "'retreived_context' missing from value. "
                "Please provide the retreived_context."
            )

        # Option to override guarded LLM call with response passed in through metadata
        if metadata.get("llm_response") is not None:
            value = metadata.get("llm_response")

        min_range_value = metadata.get("min_range_value")
        min_range_value = (
            cast(int, metadata.get("min_range_value"))
            if min_range_value is not None
            else 1
        )
        max_range_value = metadata.get("max_range_value")
        max_range_value = (
            cast(int, metadata.get("max_range_value"))
            if max_range_value is not None
            else 5
        )

        # 2. Setup the prompt
        prompt = self._llm_evaluator_prompt_generator.generate_prompt(
            user_input=user_input,
            retrieved_context=retrieved_context,
            llm_response=value,
            min_range_value=min_range_value,
            max_range_value=max_range_value,
        )
        logging.debug(f"evaluator prompt: {prompt}")

        # 3. Get the LLM response
        llm_response = self.get_llm_response(prompt)
        logging.debug(f"llm evaluator response: {llm_response}")

        try:
            llm_response = self._response_format.model_validate_json(llm_response)
        except Exception as e:
            llm_response = self._response_format.model_validate(
                {"rating": 1, "explanation": "Invalid response"}
            )

        # 4. Check the LLM response and return the result
        if llm_response.rating < self._pass_threshold:
            return FailResult(
                error_message=f'''Validation failed. The LLM Judge labeled the LLM response as "{self._fail_response}". \nEvaluator prompt: {prompt}'''
            )

        return PassResult()


if __name__ == "__main__":
    from validator.prompt_hub import RAG_CONTEXT_RELEVANCE_PROMT

    print(RAG_CONTEXT_RELEVANCE_PROMT)
